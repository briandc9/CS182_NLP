{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of Baseline Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5SrOp8qBl6q",
        "outputId": "5809abe6-9491-457c-85a6-0b409056bba8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "root_folder = \"/content/drive/My Drive/CS182-Spring2020-NLP-Project/\"\n",
        "dataset_folder = \"/content/drive/My Drive/CS182-Spring2020-NLP-Project/dataset/\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BfbnMuO7Ts1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd92a0f-a270-4db4-874c-5144ec60caaa"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv57Z5ew7K7a"
      },
      "source": [
        "import tensorflow_hub as hub \n",
        "from bert import bert_tokenization\n",
        "from tensorflow import keras\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CfmssoBFBl6x"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mITlcV9Hy4b9"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLppSRRHLBKD",
        "outputId": "42799d1d-3c71-4204-a2fe-55364a0f6c71"
      },
      "source": [
        "!pip install hyperas"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperas in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from hyperas) (5.1.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from hyperas) (2.4.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (7.6.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.19.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (3.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (4.41.1)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (5.0.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (4.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (2.6.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (2.11.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->hyperas) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (0.9.4)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->hyperas) (22.0.3)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (1.0.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt->hyperas) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas) (20.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.7.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (56.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qisXsbubzHC0"
      },
      "source": [
        "def data():\n",
        "  def bert_preprocessing(reviews, tokenizer, length):\n",
        "    tkns = []\n",
        "    msks = []\n",
        "    sids = []\n",
        "    for review in reviews:\n",
        "      tokenized = tokenizer.tokenize(review)[:length]\n",
        "      input_sequence = [\"[CLS]\"] + tokenized + [\"[SEP]\"]\n",
        "      pad = length - len(input_sequence) + 2\n",
        "      tkns += [tokenizer.convert_tokens_to_ids(input_sequence) + [0]*pad]\n",
        "      msks += [[1]*len(input_sequence) + [0]*pad]\n",
        "      sids += [[0]*(length + 2)]\n",
        "    return np.array(tkns), np.array(msks), np.array(sids)\n",
        "  length=128\n",
        "  root_folder = \"/content/drive/My Drive/CS182-Spring2020-NLP-Project/\"\n",
        "  dataset_folder = \"/content/drive/My Drive/CS182-Spring2020-NLP-Project/dataset/\"\n",
        "  d = pd.read_json(path_or_buf=dataset_folder+\"yelp_review_training_dataset.jsonl\", lines=True)\n",
        "  d['stars'].value_counts(normalize=True)\n",
        "  stars = [1, 2, 3, 4, 5]\n",
        "  small_df = pd.DataFrame()\n",
        "  for i in stars:\n",
        "    temp = d[d['stars']==i]\n",
        "    temp = temp.head(5000)\n",
        "    if i == 1 or i == 2:\n",
        "      temp['sent'] = [0] * 5000\n",
        "    else:\n",
        "      temp['sent'] = [1] * 5000\n",
        "    small_df = pd.concat([temp, small_df])\n",
        "  small_df.reset_index(drop=True)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(small_df['text'], small_df['stars'], test_size=0.3)\n",
        "  train_labels = tf.keras.utils.to_categorical(y_train.values - 1, num_classes=5)\n",
        "  test_labels = tf.keras.utils.to_categorical(y_test.values - 1, num_classes=5)\n",
        "  y_train = train_labels\n",
        "  y_test = test_labels\n",
        "\n",
        "  BertTokenizer = bert_tokenization.FullTokenizer\n",
        "  module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "  bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "  tokenizer = BertTokenizer(bert_layer.resolved_object.vocab_file.asset_path.numpy(), bert_layer.resolved_object.do_lower_case.numpy())\n",
        "\n",
        "  X_train = bert_preprocessing(X_train.values, tokenizer, length - 2)\n",
        "  X_test = bert_preprocessing(X_test.values, tokenizer, length - 2)\n",
        "  return X_train, y_train, X_test, y_test, bert_layer\n",
        "\n",
        "\n",
        "def create_models(hp):\n",
        "    length=128\n",
        "    tkns = tf.keras.Input(shape=(length,), dtype=tf.int32)\n",
        "    msks = tf.keras.Input(shape=(length,), dtype=tf.int32)\n",
        "    sids = tf.keras.Input(shape=(length,), dtype=tf.int32)\n",
        "\n",
        "    hp_units_1 = hp.Int('units', min_value=64, max_value=64, step=16)\n",
        "    do_units_1 = hp.Choice('dropout_rate_1', values=[0.2])\n",
        "    hp_units_2 = hp.Int('units_2', min_value=32, max_value=32, step=16)\n",
        "    do_units_2 = hp.Choice('dropout_rate_2', values=[0.2])\n",
        "    hp_lr = hp.Choice('learning_rate', values=[1e-4, 1e-5, 1e-6])\n",
        "\n",
        "    _, sequence_output = bert_layer([tkns, msks, sids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    net = tf.keras.layers.Dense(units=hp_units_1, activation='relu')(clf_output)\n",
        "    net = tf.keras.layers.Dropout(rate=do_units_1)(net)\n",
        "    net = tf.keras.layers.Dense(units=hp_units_2, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(rate=do_units_2)(net)\n",
        "    out = tf.keras.layers.Dense(5, activation='softmax')(net)\n",
        "    model = tf.keras.models.Model(inputs=[tkns, msks, sids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=hp_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uos6b0mawsL",
        "outputId": "d38c4d5e-337e-40da-f752-9ef58459ff55"
      },
      "source": [
        "X_train, y_train, X_test, y_test, bert_layer = data()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha7biAtrcBYg"
      },
      "source": [
        "!rm -r untitled_project"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZolnmjLSa9x7",
        "outputId": "a6e31893-6c04-4703-f1dc-8d82baa96a90"
      },
      "source": [
        "tuner = kt.Hyperband(create_models,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=3,\n",
        "                     factor=3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[6][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           65600       tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2080        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            165         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 335,209,734\n",
            "Trainable params: 335,209,733\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMoi1m4Pads2"
      },
      "source": [
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=1, validation_split=0.2, callbacks=[earlystopping], verbose=1, batch_size=16)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units is {best_hps.get('units')}, {best_hps.get('units_2')}, {best_hps.get('dropout_rate_1')}, {best_hps.get('dropout_rate_2')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhYLOJcGfxk"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0T5po7hGgic"
      },
      "source": [
        "res.history['val_']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFJZkwjZa9Ca"
      },
      "source": [
        "new_model = tf.keras.models.load_model('model.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47TLd1LqfEQA"
      },
      "source": [
        "predictions = pd.DataFrame({\"pred\": np.argmax(new_model.predict(c), axis=1), \"actual\": d})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6-skYK-7_Xj"
      },
      "source": [
        "pred = np.argmax(new_model.predict(c), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgM7Mp7n9QmI"
      },
      "source": [
        "sum(pred == np.argmax(d, axis=1))/len(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsOVJK9o7-01"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}